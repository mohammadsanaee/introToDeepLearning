{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohammadsanaee/introToDeepLearning/blob/main/1-Experiments-with-Architectures.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9Fni0HZrYwdJ"
      },
      "source": [
        "# Task: Building the application for handwritten digits recognition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xgKMaMMMTnas"
      },
      "source": [
        "## 1. Pytorch Lightning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWt0iuJbKdwY"
      },
      "source": [
        "### 1.0 Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NkgaFfs6ZHGc"
      },
      "source": [
        "**Pytorch Lightning** simplifies the programming process for training neural networks.\n",
        "\n",
        "Usually, when we program the learning process, we operate with standard procedures, regardless of the problem that we will solve. These procedures go in a certain sequence, which we will call a pipeline.\n",
        "\n",
        "For example, I can build a learning sequence like this:\n",
        "\n",
        "```\n",
        "1. Prepare and initialize the neural network, transfer it to\n",
        "     required device (gpu, cpu, tpu, npu)\n",
        "\n",
        "2. Prepare dataset\n",
        "\n",
        "3. Initialize optimizer\n",
        "\n",
        "4. Repeat n epochs:\n",
        "\n",
        "     4.1 Perform training on the training dataset during the epoch:\n",
        "         4.1.1 Take several examples (batch) from the dataset and preprocess them\n",
        "               so that they can be passed through the network,\n",
        "               transfer these examples to the one used in the learning process\n",
        "               device (gpu, cpu, tpu, npu)\n",
        "\n",
        "         4.1.2 Switch the neural network to learning mode (this is important if\n",
        "               neural network there are modules that work differently in\n",
        "               training and validation process, such as batch normalization)\n",
        "\n",
        "         4.1.3 Run a batch through the neural network and get the network's response to\n",
        "               teaching examples\n",
        "\n",
        "         4.1.4 Compare the result of the neural network with the target values\n",
        "               using the loss function\n",
        "\n",
        "         4.1.5 Zero out previously calculated weight gradients\n",
        "\n",
        "         4.1.6 Calculate new weight gradients using the inverse method\n",
        "               error propagation (loss.backward())\n",
        "\n",
        "         4.1.7 Make an optimizer step\n",
        "\n",
        "         4.1.8 Calculate metrics on the current batch\n",
        "\n",
        "         4.1.9 Update log, write learning status to file\n",
        "\n",
        "         4.1.10 Switch the network to validation mode (just in case, so that\n",
        "                statistics of batch normalizations were not accidentally spoiled somewhere)\n",
        "\n",
        "         4.1.11 Check if we have reached a plateau. If yes, then slow down\n",
        "                learning\n",
        "\n",
        "     4.2 Perform validation on the validation dataset (same as\n",
        "         training, but without optimization and without switching to training\n",
        "         mode):\n",
        "\n",
        "         4.2.1 ....\n",
        "         4.2.2 ....\n",
        "\n",
        "     4.3 Visualize several examples from the validation, training,\n",
        "         test dataset:\n",
        "\n",
        "         4.3.1 ....\n",
        "         4.3.2 ....\n",
        "\n",
        "     4.4 Save at the end of the epoch the weight of the model to a file, check if it is the best\n",
        "         model, if the model is the best, then save the weights to a file with the best weights.\n",
        "\n",
        "     4.5? ....\n",
        "\n",
        "5 ? ....\n",
        "```\n",
        "\n",
        "The model training process may differ from task to task, but most of the components are standard.\n",
        "\n",
        "If you write a very complex learning process with your hands, then you can make a random mistake (for example, very often programmers forget to switch between the training and test modes of the neural network, reset the gradients and make many other unpleasant errors). As a result, the neural network is trained incorrectly, and may not be trained at all. At the same time, it is extremely difficult to find such an error, since this is not a compilation error or division by 0: everything seems to work, but the result is not very good.\n",
        "\n",
        "**Pytorch Lightning** allows you not to write a large enough amount of code, automating the transfer of network and data to the GPU or TPU, switching between neural network modes, and adding fairly simple logging. In addition, there are a lot of useful plugins for training neural networks using PyTorch Lightning. Therefore, to solve full-fledged problems of Machine Learning, it is desirable to use this library.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "leg6cl0LTvgE"
      },
      "source": [
        "### 1.1 Library installation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SiGjexKubVKC"
      },
      "source": [
        "First, let's install PyTorch Lightning using the pip terminal command. Terminal commands can be used in Jupyter Notebooks and Colabs. To run a terminal command, you must precede the command with an exclamation mark. For example, to view the contents of a folder, you can use the ```!ls``` command."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3U4mXXKfx_o8"
      },
      "outputs": [],
      "source": [
        "!pip install -q pytorch-lightning\n",
        "\n",
        "# the -q flag allows you to significantly reduce the output when installing a package"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qUB-ipAtV0Y4"
      },
      "source": [
        "### 1.2 Imorting the modules"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n_dqtXgrF9CS"
      },
      "source": [
        "In this practice, we will use several libraries. First is ```Pytorch Lightning```. In addition, we need ```Pytorch``` itself: we will use it to define neural networks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "E_UWTkhTxi2J"
      },
      "outputs": [],
      "source": [
        "import pytorch_lightning as pl\n",
        "import torch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TspNY0teVuRe"
      },
      "source": [
        "### 1.3 Reproducible training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mWuo5OK7WZYl"
      },
      "source": [
        "In order to do A-B tests and gradually improve the learning process of the neural network, you need to make sure that the learning result is reproducible from run to run. Since pseudo-random numbers are often used in training neural networks, it is necessary that random number generators produce the same sequences from run to run. In addition, you need to switch CUDA to deterministic mode. This requirement reduces the speed of program execution, but the results of calculations become reproducible."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sByl2X8OWhrJ",
        "outputId": "6de93550-d494-46c7-f7d8-91d6a447c94a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Global seed set to 42\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "42"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "seed=42\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)\n",
        "torch.backends.cudnn.deterministic = True\n",
        "torch.backends.cudnn.benchmark = False\n",
        "\n",
        "pl.seed_everything(seed)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dZZUkIX4T6Jk"
      },
      "source": [
        "## 2. Preparing data, dataset class and data loaders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fXdzN3I3Kqc4"
      },
      "source": [
        "### 2.0 Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pfUFy_YTb722"
      },
      "source": [
        "In order to be able to pull out a training example by index and know how many training examples are contained in the dataset, a dataset is usually written. In this practice, we will use standard datasets, so we just need to initialize the standard objects of the **MNIST** class, which are implemented in the torchvision standard library.\n",
        "\n",
        "Below we initialize two datasets: training and test.\n",
        "\n",
        "In real life, for this it is necessary to implement some class that stores the data index (that is, information about where and how the data is stored), and also returns one training example upon request.\n",
        "\n",
        "In order to make a dataset class, you need to make a class with the following interface:\n",
        "\n",
        "\n",
        "\n",
        "```python\n",
        "classDataset:\n",
        "     def __init__(self):\n",
        "         # Constructor code where it usually happens\n",
        "         # collecting an index about the dataset: paths to files,\n",
        "         # about how the data is related and\n",
        "         # etc.\n",
        "         #\n",
        "         # Here it is also worth setting augmentations,\n",
        "         # which will be used in the dataset.\n",
        "\n",
        "     def __len__(self):\n",
        "         # This method is required in order to be\n",
        "         # it is possible to get the number of elements in\n",
        "         # dataset using the len() method:\n",
        "         #\n",
        "         #len(dataset)\n",
        "         #\n",
        "         # This method is used by standard\n",
        "         # data loader classes in PyTorch.\n",
        "         #\n",
        "         # The method must return a number.\n",
        "\n",
        "     def __getitem__(self, index):\n",
        "         # This method loads one\n",
        "         # example from the dataset. Here is an image and\n",
        "         # the markup is read from disk and then normally\n",
        "         # are augmented.\n",
        "         #\n",
        "         # The method must return either a tuple,\n",
        "         # either a list or a dictionary. Preferably from\n",
        "         # torch tensors.\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_zYmlW8R4XeK"
      },
      "source": [
        "### 2.1 Dataset initialization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FN0m0ec3eHFz"
      },
      "source": [
        "In this notebook, we will not write our own datasets, but will use those that are available in the `torchvision` auxiliary library. We will be interested in the `MNIST` dataset.\n",
        "\n",
        "1. Examine the interface of the `torchvision.datasets.MNIST` class. See what parameters this dataset can influence.\n",
        "\n",
        "2. Initialize training and test datasets (`torchvision.datasets.MNIST`).\n",
        "\n",
        "3. In the constructor, use the following parameters:\n",
        "     * augmentations from `torchvision.transforms`:\n",
        "         * `transforms.ToTensor()`\n",
        "         * `transforms.Normalize((0.1307,), (0.3081,))`\n",
        "\n",
        "     * `root` -- whatever you like\n",
        "     * analyze and set the rest of the arguments as you think is more correct. Do not forget that shuffling the dataset makes sense for the training set."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rIczAZDQ5ed"
      },
      "source": [
        "#### Hint: Documentation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tcAaBD7mQ953"
      },
      "source": [
        "To view the documentation of a function (or class constructor):\n",
        "\n",
        "1. Type the name of the function in the same way as it will be used in the code, but without brackets.\n",
        "2. Place a question mark `?` after the function name and\n",
        "3. execute the cell.\n",
        "\n",
        "A \"Help\" window will appear in the right window, where the documentation will be displayed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oOotwBlCq1yH"
      },
      "source": [
        "#### Hint: dataset arguments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LQ2gpdmUq865"
      },
      "source": [
        "* example path: `root='MNIST/train.pt'` for train and `root='MNIST/test.pt'` for test\n",
        "* you need to set the `download=True` argument in order for the data to be downloaded from the server\n",
        "* in order for the data to be transformed into `torch.Tensor`, you must set the `transforms=transforms.ToTensor()` argument. With the same argument, you can configure various basic augmentations available in `torchvision.transforms`. In practice, these augmentations are not recommended. We will use `albumentations` in the future."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qLB9pVGrdsRP"
      },
      "source": [
        "#### Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j8cyDu3RQG1V",
        "outputId": "4ca5d07c-c518-4386-92fc-b6d7d2333aa8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to VOC/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a80fcb9f509d4f3d98767d5cb2fa5009",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/9912422 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting VOC/MNIST/raw/train-images-idx3-ubyte.gz to VOC/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to VOC/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "01d0b007b3954b8b8dd2a6ad083acfb9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/28881 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting VOC/MNIST/raw/train-labels-idx1-ubyte.gz to VOC/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to VOC/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "650f8ad0998b4b89b91597cc7c51eb16",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/1648877 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting VOC/MNIST/raw/t10k-images-idx3-ubyte.gz to VOC/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to VOC/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aba6ae187cbd4fbd90dbbef8375a1377",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/4542 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Extracting VOC/MNIST/raw/t10k-labels-idx1-ubyte.gz to VOC/MNIST/raw\n",
            "\n"
          ]
        }
      ],
      "source": [
        "from torchvision.transforms.transforms import Resize\n",
        "\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "\n",
        "## YOUR CODE HERE\n",
        "\n",
        "train_dataset = ...\n",
        "test_dataset = ...\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iKyLg2xM9N8g",
        "outputId": "e1b79c62-bd15-410a-e693-98698e1944ac"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OK\n"
          ]
        }
      ],
      "source": [
        "#@title Unit Test\n",
        "\n",
        "# Checking the datasets classes\n",
        "assert(isinstance(train_dataset, torchvision.datasets.MNIST))\n",
        "assert(isinstance(test_dataset, torchvision.datasets.MNIST))\n",
        "\n",
        "# Checking volumes of the datasets\n",
        "assert(len(train_dataset) == 60000)\n",
        "assert(len(test_dataset) == 10000)\n",
        "\n",
        "# Checking outputs of the datasets\n",
        "assert(isinstance(train_dataset[0][0], torch.Tensor))\n",
        "assert(isinstance(train_dataset[0][1], int))\n",
        "\n",
        "# Checking dimentionalities\n",
        "assert(train_dataset[0][0].shape == torch.Size((1, 28, 28)))\n",
        "assert(train_dataset[0][1] == 5)\n",
        "\n",
        "print('OK')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_0GI7bIh_oc"
      },
      "source": [
        "#### Checking the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJ41Nzp7612c"
      },
      "source": [
        "It is very important to check that the dataset works as it should. To do this, it is worth visualizing the picture if we are working with images, looking at the text if we are dealing with texts, or listening to the audio track if sound is being analyzed.\n",
        "\n",
        "You need to check that the image is displayed correctly, and also that the label, if any, matches the image.\n",
        "\n",
        "You should also pay attention to data sizes (the `.shape` method). Most of the errors can be found by analyzing only the size of the data.\n",
        "\n",
        "It's worth looking at some data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 165
        },
        "id": "X4hJ-43sgeKo",
        "outputId": "16cc64bb-4cd2-4b42-9669-cdb52b34e582"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Число примеров в тренировочном и тестовом датасете:\n",
            "60000 10000\n",
            "Размер изображения. ОБРАТИТЕ ВНИМАНИЕ, что первым стоит число каналов:\n",
            "torch.Size([1, 28, 28])\n",
            "Класс изображения и название класса:\n",
            "5 5 - five\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\n<svg xmlns:xlink=\"http://www.w3.org/1999/xlink\" width=\"300.237pt\" height=\"297.190125pt\" viewBox=\"0 0 300.237 297.190125\" xmlns=\"http://www.w3.org/2000/svg\" version=\"1.1\">\n <metadata>\n  <rdf:RDF xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\n   <cc:Work>\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\n    <dc:date>2023-04-14T13:23:03.635434</dc:date>\n    <dc:format>image/svg+xml</dc:format>\n    <dc:creator>\n     <cc:Agent>\n      <dc:title>Matplotlib v3.7.0, https://matplotlib.org/</dc:title>\n     </cc:Agent>\n    </dc:creator>\n   </cc:Work>\n  </rdf:RDF>\n </metadata>\n <defs>\n  <style type=\"text/css\">*{stroke-linejoin: round; stroke-linecap: butt}</style>\n </defs>\n <g id=\"figure_1\">\n  <g id=\"patch_1\">\n   <path d=\"M -0 297.190125 \nL 300.237 297.190125 \nL 300.237 0 \nL -0 0 \nz\n\"/>\n  </g>\n  <g id=\"axes_1\">\n   <g id=\"patch_2\">\n    <path d=\"M 26.925 273.312 \nL 293.037 273.312 \nL 293.037 7.2 \nL 26.925 7.2 \nz\n\"/>\n   </g>\n   <g clip-path=\"url(#pe81876eea0)\">\n    <image xlink:href=\"data:image/png;base64,\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\" id=\"imagea52be10445\" transform=\"scale(1 -1) translate(0 -266.4)\" x=\"26.925\" y=\"-6.912\" width=\"266.4\" height=\"266.4\"/>\n   </g>\n   <g id=\"matplotlib.axis_1\">\n    <g id=\"xtick_1\">\n     <g id=\"line2d_1\">\n      <defs>\n       <path id=\"ma26174a007\" d=\"M 0 0 \nL 0 3.5 \n\" style=\"stroke: #ffffff; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#ma26174a007\" x=\"31.677\" y=\"273.312\" style=\"fill: #ffffff; stroke: #ffffff; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_1\">\n      <!-- 0 -->\n      <g style=\"fill: #ffffff\" transform=\"translate(28.49575 287.910437) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-30\" d=\"M 2034 4250 \nQ 1547 4250 1301 3770 \nQ 1056 3291 1056 2328 \nQ 1056 1369 1301 889 \nQ 1547 409 2034 409 \nQ 2525 409 2770 889 \nQ 3016 1369 3016 2328 \nQ 3016 3291 2770 3770 \nQ 2525 4250 2034 4250 \nz\nM 2034 4750 \nQ 2819 4750 3233 4129 \nQ 3647 3509 3647 2328 \nQ 3647 1150 3233 529 \nQ 2819 -91 2034 -91 \nQ 1250 -91 836 529 \nQ 422 1150 422 2328 \nQ 422 3509 836 4129 \nQ 1250 4750 2034 4750 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_2\">\n     <g id=\"line2d_2\">\n      <g>\n       <use xlink:href=\"#ma26174a007\" x=\"79.197\" y=\"273.312\" style=\"fill: #ffffff; stroke: #ffffff; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_2\">\n      <!-- 5 -->\n      <g style=\"fill: #ffffff\" transform=\"translate(76.01575 287.910437) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-35\" d=\"M 691 4666 \nL 3169 4666 \nL 3169 4134 \nL 1269 4134 \nL 1269 2991 \nQ 1406 3038 1543 3061 \nQ 1681 3084 1819 3084 \nQ 2600 3084 3056 2656 \nQ 3513 2228 3513 1497 \nQ 3513 744 3044 326 \nQ 2575 -91 1722 -91 \nQ 1428 -91 1123 -41 \nQ 819 9 494 109 \nL 494 744 \nQ 775 591 1075 516 \nQ 1375 441 1709 441 \nQ 2250 441 2565 725 \nQ 2881 1009 2881 1497 \nQ 2881 1984 2565 2268 \nQ 2250 2553 1709 2553 \nQ 1456 2553 1204 2497 \nQ 953 2441 691 2322 \nL 691 4666 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-35\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_3\">\n     <g id=\"line2d_3\">\n      <g>\n       <use xlink:href=\"#ma26174a007\" x=\"126.717\" y=\"273.312\" style=\"fill: #ffffff; stroke: #ffffff; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_3\">\n      <!-- 10 -->\n      <g style=\"fill: #ffffff\" transform=\"translate(120.3545 287.910437) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-31\" d=\"M 794 531 \nL 1825 531 \nL 1825 4091 \nL 703 3866 \nL 703 4441 \nL 1819 4666 \nL 2450 4666 \nL 2450 531 \nL 3481 531 \nL 3481 0 \nL 794 0 \nL 794 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_4\">\n     <g id=\"line2d_4\">\n      <g>\n       <use xlink:href=\"#ma26174a007\" x=\"174.237\" y=\"273.312\" style=\"fill: #ffffff; stroke: #ffffff; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_4\">\n      <!-- 15 -->\n      <g style=\"fill: #ffffff\" transform=\"translate(167.8745 287.910437) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_5\">\n     <g id=\"line2d_5\">\n      <g>\n       <use xlink:href=\"#ma26174a007\" x=\"221.757\" y=\"273.312\" style=\"fill: #ffffff; stroke: #ffffff; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_5\">\n      <!-- 20 -->\n      <g style=\"fill: #ffffff\" transform=\"translate(215.3945 287.910437) scale(0.1 -0.1)\">\n       <defs>\n        <path id=\"DejaVuSans-32\" d=\"M 1228 531 \nL 3431 531 \nL 3431 0 \nL 469 0 \nL 469 531 \nQ 828 903 1448 1529 \nQ 2069 2156 2228 2338 \nQ 2531 2678 2651 2914 \nQ 2772 3150 2772 3378 \nQ 2772 3750 2511 3984 \nQ 2250 4219 1831 4219 \nQ 1534 4219 1204 4116 \nQ 875 4013 500 3803 \nL 500 4441 \nQ 881 4594 1212 4672 \nQ 1544 4750 1819 4750 \nQ 2544 4750 2975 4387 \nQ 3406 4025 3406 3419 \nQ 3406 3131 3298 2873 \nQ 3191 2616 2906 2266 \nQ 2828 2175 2409 1742 \nQ 1991 1309 1228 531 \nz\n\" transform=\"scale(0.015625)\"/>\n       </defs>\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"xtick_6\">\n     <g id=\"line2d_6\">\n      <g>\n       <use xlink:href=\"#ma26174a007\" x=\"269.277\" y=\"273.312\" style=\"fill: #ffffff; stroke: #ffffff; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_6\">\n      <!-- 25 -->\n      <g style=\"fill: #ffffff\" transform=\"translate(262.9145 287.910437) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"matplotlib.axis_2\">\n    <g id=\"ytick_1\">\n     <g id=\"line2d_7\">\n      <defs>\n       <path id=\"ma971b686fe\" d=\"M 0 0 \nL -3.5 0 \n\" style=\"stroke: #ffffff; stroke-width: 0.8\"/>\n      </defs>\n      <g>\n       <use xlink:href=\"#ma971b686fe\" x=\"26.925\" y=\"11.952\" style=\"fill: #ffffff; stroke: #ffffff; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_7\">\n      <!-- 0 -->\n      <g style=\"fill: #ffffff\" transform=\"translate(13.5625 15.751219) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-30\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_2\">\n     <g id=\"line2d_8\">\n      <g>\n       <use xlink:href=\"#ma971b686fe\" x=\"26.925\" y=\"59.472\" style=\"fill: #ffffff; stroke: #ffffff; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_8\">\n      <!-- 5 -->\n      <g style=\"fill: #ffffff\" transform=\"translate(13.5625 63.271219) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-35\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_3\">\n     <g id=\"line2d_9\">\n      <g>\n       <use xlink:href=\"#ma971b686fe\" x=\"26.925\" y=\"106.992\" style=\"fill: #ffffff; stroke: #ffffff; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_9\">\n      <!-- 10 -->\n      <g style=\"fill: #ffffff\" transform=\"translate(7.2 110.791219) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_4\">\n     <g id=\"line2d_10\">\n      <g>\n       <use xlink:href=\"#ma971b686fe\" x=\"26.925\" y=\"154.512\" style=\"fill: #ffffff; stroke: #ffffff; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_10\">\n      <!-- 15 -->\n      <g style=\"fill: #ffffff\" transform=\"translate(7.2 158.311219) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-31\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_5\">\n     <g id=\"line2d_11\">\n      <g>\n       <use xlink:href=\"#ma971b686fe\" x=\"26.925\" y=\"202.032\" style=\"fill: #ffffff; stroke: #ffffff; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_11\">\n      <!-- 20 -->\n      <g style=\"fill: #ffffff\" transform=\"translate(7.2 205.831219) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use xlink:href=\"#DejaVuSans-30\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n    <g id=\"ytick_6\">\n     <g id=\"line2d_12\">\n      <g>\n       <use xlink:href=\"#ma971b686fe\" x=\"26.925\" y=\"249.552\" style=\"fill: #ffffff; stroke: #ffffff; stroke-width: 0.8\"/>\n      </g>\n     </g>\n     <g id=\"text_12\">\n      <!-- 25 -->\n      <g style=\"fill: #ffffff\" transform=\"translate(7.2 253.351219) scale(0.1 -0.1)\">\n       <use xlink:href=\"#DejaVuSans-32\"/>\n       <use xlink:href=\"#DejaVuSans-35\" x=\"63.623047\"/>\n      </g>\n     </g>\n    </g>\n   </g>\n   <g id=\"patch_3\">\n    <path d=\"M 26.925 273.312 \nL 26.925 7.2 \n\" style=\"fill: none; stroke: #ffffff; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_4\">\n    <path d=\"M 293.037 273.312 \nL 293.037 7.2 \n\" style=\"fill: none; stroke: #ffffff; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_5\">\n    <path d=\"M 26.925 273.312 \nL 293.037 273.312 \n\" style=\"fill: none; stroke: #ffffff; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n   <g id=\"patch_6\">\n    <path d=\"M 26.925 7.2 \nL 293.037 7.2 \n\" style=\"fill: none; stroke: #ffffff; stroke-width: 0.8; stroke-linejoin: miter; stroke-linecap: square\"/>\n   </g>\n  </g>\n </g>\n <defs>\n  <clipPath id=\"pe81876eea0\">\n   <rect x=\"26.925\" y=\"7.2\" width=\"266.112\" height=\"266.112\"/>\n  </clipPath>\n </defs>\n</svg>\n",
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "print('Number of examples in training and test dataset:')\n",
        "print(len(train_dataset), len(test_dataset))\n",
        "print('Image size. NOTE that number of channels comes first:')\n",
        "print(train_dataset[0][0].shape)\n",
        "plt.imshow(train_dataset[0][0][0]) # draw the image\n",
        "print('Image class and class name:')\n",
        "print(train_dataset[0][1], train_dataset.classes[train_dataset[0][1]])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qqVoQYo3Fiwe"
      },
      "source": [
        "### 2.2 Creating data loaders"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2fH-VFCadTg7"
      },
      "source": [
        "The datasets are defined. In order to automatically extract from datasets\n",
        "batches (small bundles of data that are ready to be passed to the input of the neural network), there is a special class in pytorch: DataLoader (loader). It can be\n",
        "flexible enough to configure:\n",
        "\n",
        "* select the batch size to be\n",
        "pulled out of the DataLoader,\n",
        "\n",
        "* choose how many parallel processes to use for\n",
        "batch processing (this significantly reduces the time for data processing, including\n",
        "when augmentations are used).\n",
        "\n",
        "* shuffle data\n",
        "(this is useful for training data: then the batches that enter the network\n",
        "will vary).\n",
        "\n",
        "* discard last batch on load\n",
        "\n",
        "Discarding the last batch in the training dataset is very important,\n",
        "since it can be small, the statistics that are calculated from it can be\n",
        "unrepresentative. As a result, batch normalizations will be corrupted.\n",
        "To avoid this, the ```drop_last``` parameter is provided."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALud-qiffruc"
      },
      "source": [
        "#### Exercise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZBqbLY_LYyl"
      },
      "source": [
        "Configure data loaders. Set: batch size 32, number of processing threads (workers) 8.\n",
        "\n",
        "Set the training data to random shuffle and discard the last batch."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51hZLnk1LcSJ"
      },
      "source": [
        "##### Hint: loader arguments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ezsoPcYrLfyN"
      },
      "source": [
        "For the training dataset, set:\n",
        "* `batch_size` equal to `32`\n",
        "* `num_workers` to `8`\n",
        "* `shuffle` to `True`\n",
        "* `drop_last` to `True`\n",
        "\n",
        "For a test dataset, set:\n",
        "* `batch_size` equal to `32`\n",
        "* `num_workers` to `8`\n",
        "* `shuffle` to `False`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bFjF5lk7MFTT"
      },
      "source": [
        "#### Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cyMuUgQodQcm"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import DataLoader\n",
        "\n",
        "train_loader = ... #YOUR CODE HERE\n",
        "test_loader  = ... #YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dtOO1GnOOEWj"
      },
      "source": [
        "## 3. Defining the error function and metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MS9UeP4eOSCo"
      },
      "source": [
        "### 3.1 Loss functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UdVuVhBtjcTG"
      },
      "source": [
        "For any task, it is necessary to define a loss function, with the help of which the model will be trained. The loss function can be set by hand, or you can use a ready-made one.\n",
        "\n",
        "In this practice, we will use the already implemented loss function: ```torch.nn.CrossEntropyLoss```.\n",
        "\n",
        "We will do this in the first place because this loss function combines `torch.nn.SoftMax` and the cross-entropy `torch.nn.NLLloss` itself, which prevents numerical instability when calculating the loss. It is rare, but in some cases it can spoil the learning process.\n",
        "\n",
        "In case you need to use some custom loss function, you can implement the `loss` function, which will return a `torch.Tensor` consisting of a single number. This is important so that you can later use `backward` for gradient calculations."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4nE3JOaOeIM"
      },
      "source": [
        "#### Exercise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SfO2dlsaOf_P"
      },
      "source": [
        "Initialize the loss ```torch.nn.CrossEntropyLoss```."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jpP4pHsjg-0Y"
      },
      "source": [
        "#### Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RJGGQWwiwHi4"
      },
      "outputs": [],
      "source": [
        "loss = ... # YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u89Q_Ihk7__i",
        "outputId": "3e45d8e7-e02a-4041-8020-c595268b78f1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OK\n"
          ]
        }
      ],
      "source": [
        "#@title Unit Test\n",
        "import torch\n",
        "\n",
        "for index in range(1000):\n",
        "    x = torch.randn([10, 10])\n",
        "    y = torch.randint(0, 10, size=[10])\n",
        "\n",
        "    assert((loss(x, y) - torch.nn.functional.cross_entropy(x, y)).abs().mean() < 1.0e-8)\n",
        "\n",
        "print('OK')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "38Az-VUQOvAU"
      },
      "source": [
        "### 3.2 Metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UmZ1JZ_q5zm"
      },
      "source": [
        "For a normal person, loss is not informative. In order for us to understand how well or poorly the model actually works, we need to decide on a metric. We will use the \"accuracy\" metric.\n",
        "\n",
        "To calculate metrics, there is a library [torchmetrics] (https://github.com/PyTorchLightning/metrics). We will use this library in our projects.\n",
        "\n",
        "In order to implement a metric for PyTorch Lightning, you need to implement a class inherited from `trochmetrics.Metrics`, in which to implement three methods:\n",
        "\n",
        "* ```__init__(self)``` -- constructor. In the constructor, you must initialize the parent class with ```super().__init__()```. It is also necessary to initialize all auxiliary variables here (in our case, the ```correct``` fields, the number of correctly guessed objects, and the ```total``` field, the total number of classified objects). Variables must be declared in a special way: using the ```self.add_state``` method. For example, to define a field ```correct```, you need to write in the constructor: ```self.add_state(\"correct\", default=torch.tensor(0.0), dist_reduce_fx=\"sum\")```. The last parameter is needed for multi-threaded learning.\n",
        "\n",
        "* ```update(self, preds, targets)``` -- update when new neural network results are received. The input is tensors with the results of the network and target variables (batches). Usually this method is to update auxiliary variables. In our case, we need to add the number of guessed data to ```self.correct``` and the total number of objects in the batch to ```self.total```, which were declared in the constructor. Shouldn't return anything.\n",
        "\n",
        "* ```compute(self)``` is a method that calculates the metric value from auxiliary variables. In our case -- divides ```self.correct``` by ```self.total```."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sgr3wzOS0pqA"
      },
      "outputs": [],
      "source": [
        "import torchmetrics\n",
        "\n",
        "# YOUR CODE HERE\n",
        "\n",
        "class Accuracy(torchmetrics.Metric):\n",
        "    def __init__(self):\n",
        "        ...\n",
        "\n",
        "    def update(self, preds, target):\n",
        "        ...\n",
        "\n",
        "    def compute(self):\n",
        "        ..."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5Z38_i1k1ft6",
        "outputId": "bdac7574-2a25-4310-8587-4e24b62f4723"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/mike/miniconda3/envs/torch/lib/python3.10/site-packages/torchmetrics/utilities/prints.py:36: UserWarning: Torchmetrics v0.9 introduced a new argument class property called `full_state_update` that has\n",
            "                not been set for this class (Accuracy). The property determines if `update` by\n",
            "                default needs access to the full metric state. If this is not the case, significant speedups can be\n",
            "                achieved and we recommend setting this to `False`.\n",
            "                We provide an checking function\n",
            "                `from torchmetrics.utilities import check_forward_full_state_property`\n",
            "                that can be used to check if the `full_state_update=True` (old and potential slower behaviour,\n",
            "                default for now) or if `full_state_update=False` can be used safely.\n",
            "                \n",
            "  warnings.warn(*args, **kwargs)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OK\n"
          ]
        }
      ],
      "source": [
        "#@title Unit test\n",
        "\n",
        "correct = 0\n",
        "all = 0\n",
        "\n",
        "acc = Accuracy()\n",
        "\n",
        "for index in range(1000):\n",
        "    preds = torch.randn([1000, 100])\n",
        "    targs = torch.randint(0, 100, size=[1000])\n",
        "    acc.update(preds, targs)\n",
        "\n",
        "    correct += (preds.argmax(dim=1) == targs).float().sum()\n",
        "    all += targs.numel()\n",
        "\n",
        "    assert((acc.compute() - correct / all).abs().mean() < 1.0e-8)\n",
        "\n",
        "# acc.update(preds, targs)\n",
        "# assert(acc.compute() == 1.0 / 3.0)\n",
        "\n",
        "print('OK')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4y8Jk4hFn7qo"
      },
      "source": [
        "## 4. Building the network and getting ready to train it"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U3K9vX8XxhQs"
      },
      "source": [
        "### 4.1. Architecture preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JmwyUvjCeWNv"
      },
      "source": [
        "#### Exercise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s1bh9ZPY2SL4"
      },
      "source": [
        "We will make a simple two-layer fully connected network and train it. This network will have the following modules:\n",
        "\n",
        "```\n",
        "*BatchNorm1d\n",
        "Linear 28 * 28 -> n_hidden\n",
        "activation: ReLU, LeakyReLU, ELU, Tanh\n",
        "*BatchNorm1d\n",
        "Linear n_hidden -> 10\n",
        "```\n",
        "\n",
        "In order to make a neural network architecture, it is necessary to define a class inherited from `torch.Module`, in which the following methods must be defined:\n",
        "\n",
        "* ```__init__(self)``` -- constructor to define the neural network modules to be used (same as in PyTorch). In addition, here it is necessary to initialize the classes of metrics and loss functions. Before declaring modules, you must call ```super().__init__()``` to initialize the parent class that provides the main functionality.\n",
        "\n",
        "* ```forward(self, input)``` -- pass through the network forward. Here you need to make predictions from ```input``` using modules defined in the constructor (nothing special here either, just like in PyTorch)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eD-5rFJ0eakB"
      },
      "source": [
        "#### Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wG7sKsGr0g80"
      },
      "outputs": [],
      "source": [
        "class Network(torch.nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        ... # YOUR CODE HERE\n",
        "\n",
        "    def forward(self, x):\n",
        "        ... # YOUR CODE HERE\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GQjTPgoN2ZRa",
        "outputId": "ed06df79-778a-4bbe-d955-4406b117897e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "OK\n"
          ]
        }
      ],
      "source": [
        "#@title Unit test\n",
        "\n",
        "x = torch.zeros(10, 1, 28, 28)\n",
        "net = Network()\n",
        "res = net.forward(x)\n",
        "\n",
        "print('OK')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XllTf9FCxudA"
      },
      "source": [
        "### 4.2. Preparing the Pytorch Lightning Module"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4hLn2tX2KWY"
      },
      "source": [
        "In order to prepare the network for training on PyTorch Lightning, you need to make a class inherited from ```pl.LightningModule```.\n",
        "\n",
        "In this class, you need to define:\n",
        "* As in PyTorch:\n",
        "     * `__init__(self)`\n",
        "     * `forward(self, x)`\n",
        "\n",
        "In our case, for convenience, it makes sense to initialize the `self.model` Pytorch field in `__init__` with the model specified earlier, and use the same method from `self.model` in the forward method.\n",
        "\n",
        "* Minimal kit for PyTorch Lightning:\n",
        "   * ```training_step(self, batch, batch_idx)``` -- here, from network predictions made using the forward method, loss functions and metrics are calculated and logged. In order to log some value, you need to do: ```self.log('name_of_value/train', value, on_epoch=True)``` The function must return the value of the loss.\n",
        "   * ```configure_optimizers(self)```. This is where the optimizers must be defined. For example, I can define an optimizer like this: ```optim = torch.optim.Adam(self.parameters(), lr=1.0e-3)```. It is also convenient to define schedulers for optimizers here. For example, a scheduler could be defined like this: ```sched = torch.optim.lr_scheduler.ReduceLROnPlateau(optim, mode='max', factor=0.3, verbose=True)```. The function must return a list of optimizers or a single optimizer (if there is only one optimizer).\n",
        "\n",
        "* In order to be able to calculate the metrics for validation, we will also implement\n",
        "   * ```validation_step(self, batch, batch_idx)``` -- same as training_step, only for validation set. This function may not return anything."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YQKUI7-eyIY"
      },
      "source": [
        "#### Exercise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NVdK2rmFe1Wz"
      },
      "source": [
        "Populate the Pytorch Lightning class as described above"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xiRHkP_kfCaO"
      },
      "source": [
        "#### Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "it4FyRtex8tG"
      },
      "outputs": [],
      "source": [
        "class PLModel(pl.LightningModule):\n",
        "    def __init__(self,\n",
        "                 model):\n",
        "\n",
        "      super().__init__()\n",
        "      ... # YOUR CODE HERE\n",
        "\n",
        "\n",
        "    def forward(self, input):\n",
        "      return ... # YOUR CODE HERE\n",
        "\n",
        "\n",
        "    def training_step(self, batch, batch_idx):\n",
        "      ... # YOUR CODE HERE\n",
        "      return loss_value\n",
        "\n",
        "    def validation_step(self, batch, batch_idx):\n",
        "      ... # YOUR CODE HERE\n",
        "      return val_accuracy\n",
        "\n",
        "\n",
        "    def configure_optimizers(self):\n",
        "      ... # YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jO98eT5ix7jT"
      },
      "source": [
        "## 5. Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-9URMzgnyCJa"
      },
      "source": [
        "### 5.1. TensorBoard"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I6YpLvwDiJf_"
      },
      "source": [
        "**Tensorboard** is the standard way to track learning progress. In TensorBoard, you can plot metrics and loss functions, draw pictures and track how they change over time. PyTorch Lightning automatically writes logs to TensorBoard."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "6s-hywaIX01n"
      },
      "outputs": [],
      "source": [
        "#@title Launching the Tensorboard\n",
        "\n",
        "import os\n",
        "\n",
        "if not os.path.exists('lightning_logs'):\n",
        "    os.mkdir('lightning_logs')\n",
        "\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir lightning_logs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7qiF7pCOyGgJ"
      },
      "source": [
        "### 5.2. Start training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hNJwnOpWi8jm"
      },
      "source": [
        "By default, Pytorch Lightning uses not the most convenient and informative progress bar. Therefore, below we have made a more interesting progress bar, which allows you to track the learning process more conveniently and does not load the browser."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "UxHlhI6txWBo"
      },
      "outputs": [],
      "source": [
        "#@title Custom ProgressBar Code (Code is complex and irrelevant to the task)\n",
        "\n",
        "import sys\n",
        "import math\n",
        "import time\n",
        "import datetime\n",
        "import copy\n",
        "import numpy\n",
        "import os\n",
        "\n",
        "try:\n",
        "    from IPython.display import display, update_display\n",
        "except:\n",
        "    pass\n",
        "\n",
        "def isnotebook():\n",
        "    try:\n",
        "        shell = get_ipython().__class__.__name__\n",
        "        module = get_ipython().__class__.__module__\n",
        "        if module == \"google.colab._shell\":\n",
        "            return True\n",
        "        if shell == 'ZMQInteractiveShell':\n",
        "            return True   # Jupyter notebook or qtconsole\n",
        "        elif shell == 'TerminalInteractiveShell':\n",
        "            return False  # Terminal running IPython\n",
        "        else:\n",
        "            return False  # Other type (?)\n",
        "    except NameError:\n",
        "        return False\n",
        "\n",
        "def nlines(text):\n",
        "    return text.count('\\n') + 1\n",
        "\n",
        "status = {\n",
        "    'Info': '5/10 [######9     ] 59%',\n",
        "    'Time/D': 0.9999999999999999999999,\n",
        "    'Time/B': 0.1238476123864126345187245,\n",
        "    'Time/AvgD': 0.812341348176234987162349817643,\n",
        "    'Time/AvgB': 0.8561283745187634581726345,\n",
        "    'Loss/L1': 112398746192834619827364,\n",
        "    'Loss/L2': 19234618623.0,\n",
        "    'Loss/L2.5': 1,\n",
        "    'Loss/L3': 0.8126347152,\n",
        "    'Loss/L4': 10,\n",
        "    'Loss/L5': numpy.inf,\n",
        "    'Metrics/One': 0.888888,\n",
        "    'Metrics/Two': 123987.0,\n",
        "    'Metrics/Three': 1.0\n",
        "}\n",
        "\n",
        "\n",
        "def textcolor(style=None, color=None):\n",
        "    if color is None:\n",
        "        color = 0\n",
        "    else:\n",
        "        color_code = 30 + color\n",
        "    if style is None:\n",
        "        style_code = 0\n",
        "    else:\n",
        "        style_code = style\n",
        "    return '\\033[' + str(style_code) + ';' + str(color_code) + 'm', '\\033[' + str(0) + ';' + str(0) + 'm'\n",
        "\n",
        "\n",
        "def format_status(inp):\n",
        "    if isinstance(inp, (dict)):\n",
        "        for key in inp:\n",
        "            inp[key] = format_status(inp[key])\n",
        "\n",
        "    if isinstance(inp, (list, tuple)):\n",
        "        for index in range(len(inp)):\n",
        "            inp[index] = format_status(inp[index])\n",
        "\n",
        "    if isinstance(inp, torch.Tensor):\n",
        "        inp = inp.detach().cpu().numpy()\n",
        "\n",
        "    if isinstance(inp, int):\n",
        "        if abs(inp) > 10 ** 6:\n",
        "            return '{:.3e}'.format(inp)\n",
        "        else:\n",
        "            return '{:d}'.format(inp)\n",
        "\n",
        "    if isinstance(inp, float):\n",
        "        if abs(inp) > 10 ** 6:\n",
        "            return '{:.3e}'.format(inp)\n",
        "        elif abs(inp) < 10 ** -6:\n",
        "            return '{:.3e}'.format(inp)\n",
        "        else:\n",
        "            return '{:.6f}'.format(inp)\n",
        "\n",
        "    return inp\n",
        "\n",
        "\n",
        "def colorize_string(string, colors, padding=0):\n",
        "    indice = []\n",
        "    for color in colors:\n",
        "        indice.append(colors[0])\n",
        "\n",
        "    substrings = []\n",
        "    last_index = 0\n",
        "    for color in colors:\n",
        "        index = color[0] + padding\n",
        "        substrings.append(string[last_index:index])\n",
        "        substrings.append(color[1])\n",
        "        last_index = index\n",
        "    substrings.append(string[last_index:])\n",
        "    return ''.join(substrings)\n",
        "\n",
        "\n",
        "def view_status(inp, display_len=80):\n",
        "    separator = ' | '\n",
        "    strings = ['']\n",
        "    colors = [[]]\n",
        "    color_index = 0\n",
        "\n",
        "    maxlen = 0\n",
        "    for key in inp:\n",
        "        maxlen = max(len(str(key)), maxlen)\n",
        "\n",
        "    for key in inp:\n",
        "\n",
        "        start, end = textcolor(style=1, color=color_index + 1)\n",
        "        colors[-1].append((len(strings[-1]), start))\n",
        "        strings[-1] += ('{:>' + str(maxlen) + 's} ').format(key)\n",
        "        colors[-1].append((len(strings[-1]), end))\n",
        "\n",
        "        if isinstance(inp[key], (list, tuple)):\n",
        "            strings[-1] += separator.join(inp[key])\n",
        "\n",
        "        elif isinstance(inp[key], dict):\n",
        "            pos = len(strings[-1])\n",
        "            subres = []\n",
        "\n",
        "            for subkey in inp[key]:\n",
        "                start, end = textcolor(style=3, color=color_index + 1)\n",
        "                colors[-1].append((pos, start))\n",
        "                colors[-1].append((pos + len(subkey), end))\n",
        "                subres.append(subkey + ': ' + str(inp[key][subkey]))\n",
        "                pos = pos + len(subkey) + len(': ') + len(str(inp[key][subkey])) + len(separator)\n",
        "            strings[-1] += separator.join(subres)\n",
        "\n",
        "        else:\n",
        "            strings[-1] += str(inp[key])\n",
        "\n",
        "        strings.append('')\n",
        "        colors.append([])\n",
        "\n",
        "        color_index += 1\n",
        "        color_index %= 6\n",
        "\n",
        "    new_strings = []\n",
        "    new_colors = []\n",
        "    new_strings.append('=' * display_len)\n",
        "    for index in range(len(strings)):\n",
        "        string = strings[index]\n",
        "        str_colors = colors[index]\n",
        "        position = 0\n",
        "        color_index = 0\n",
        "        padding = 0\n",
        "        while len(string) > 0:\n",
        "            splitter_location = -1\n",
        "\n",
        "            if len(string) > display_len:\n",
        "                splitter_location = string[:display_len].rfind(' | ')\n",
        "\n",
        "            split_colors = []\n",
        "\n",
        "            if splitter_location > 0:\n",
        "                string_end = splitter_location\n",
        "            else:\n",
        "                string_end = min(display_len, len(string))\n",
        "            while color_index < len(colors[index]) and colors[index][color_index][0] - position < string_end - padding:\n",
        "                split_colors.append(list(colors[index][color_index]))\n",
        "                split_colors[-1][0] -= position\n",
        "                color_index += 1\n",
        "\n",
        "            if len(string) < display_len:\n",
        "                to_print = string\n",
        "                to_print = to_print + ' ' * (display_len - len(to_print))\n",
        "                new_strings.append(colorize_string(to_print, split_colors, padding=padding))\n",
        "                break\n",
        "\n",
        "            elif splitter_location > 0:\n",
        "                to_print = string[:splitter_location]\n",
        "                to_print = to_print + ' ' * (display_len - len(to_print))\n",
        "                new_strings.append(colorize_string(to_print, split_colors, padding=padding))\n",
        "                split_colors = []\n",
        "                string = ' ' * (maxlen + 1) + string[splitter_location + 3:]\n",
        "                position += splitter_location + 3 - padding\n",
        "                padding = maxlen + 1\n",
        "\n",
        "            else:\n",
        "                to_print = string[:string_end]\n",
        "                to_print = to_print + ' ' * (display_len - len(to_print))\n",
        "                new_strings.append(colorize_string(to_print, split_colors, padding=padding))\n",
        "                split_colors = []\n",
        "                string = ' ' * (maxlen + 1) + string[string_end:]\n",
        "                position += string_end - padding\n",
        "                padding = maxlen + 1\n",
        "\n",
        "    new_strings.append('=' * display_len)\n",
        "    return '\\n'.join(new_strings)\n",
        "\n",
        "\n",
        "def dict_to_multidict(status):\n",
        "    decomposed_status = {}\n",
        "    for key in list(status.keys()):\n",
        "        key_parts = key.split('/')\n",
        "        if len(key_parts) > 2:\n",
        "            continue\n",
        "\n",
        "        if len(key_parts) > 1:\n",
        "            superkey = key_parts[0]\n",
        "            subkey = '/'.join(key_parts[1:])\n",
        "\n",
        "            if superkey not in decomposed_status:\n",
        "                decomposed_status[superkey] = {}\n",
        "\n",
        "            decomposed_status[superkey][subkey] = status[key]\n",
        "        else:\n",
        "            decomposed_status[key] = status[key]\n",
        "\n",
        "    return decomposed_status\n",
        "\n",
        "\n",
        "def isnotebook():\n",
        "    try:\n",
        "        shell = get_ipython().__class__.__name__\n",
        "        module = get_ipython().__class__.__module__\n",
        "        if module == \"google.colab._shell\":\n",
        "            return True\n",
        "        if shell == 'ZMQInteractiveShell':\n",
        "            return True   # Jupyter notebook or qtconsole\n",
        "        elif shell == 'TerminalInteractiveShell':\n",
        "            return False  # Terminal running IPython\n",
        "        else:\n",
        "            return False  # Other type (?)\n",
        "    except NameError:\n",
        "        return False\n",
        "\n",
        "\n",
        "def nlines(text):\n",
        "    return text.count('\\n') + 1\n",
        "\n",
        "\n",
        "def get_width():\n",
        "    try:\n",
        "        return os.get_terminal_size()[0] - 1\n",
        "    except :\n",
        "        return 100\n",
        "\n",
        "\n",
        "\n",
        "class StageProgressBar:\n",
        "    def __init__(self, width_function=None, display_id='ep{}'.format(0), is_ipython=None):\n",
        "        self.width_function = width_function\n",
        "\n",
        "\n",
        "        self.last_vals = None\n",
        "        self.finalized = False\n",
        "        self.started = False\n",
        "\n",
        "        self.is_ipython = isnotebook() if is_ipython is None else is_ipython\n",
        "        self.display_id = display_id\n",
        "\n",
        "    def __str__(self):\n",
        "        status = format_status(self.last_vals)\n",
        "        to_view = view_status(dict_to_multidict(status), display_len=self.width)\n",
        "        return to_view\n",
        "\n",
        "    def display(self, content):\n",
        "        if not self.is_ipython:\n",
        "            print(content, end='')\n",
        "            print('\\033[' + str(nlines(content)) + 'A')\n",
        "        else:\n",
        "            # print(self.display_id)\n",
        "            update_display({'text/plain': content}, display_id=self.display_id, raw=True)\n",
        "\n",
        "    def __del__(self):\n",
        "        self.finalize()\n",
        "\n",
        "    def update(self, vals):\n",
        "        if self.finalized:\n",
        "            return\n",
        "\n",
        "        self.width = self.width_function()\n",
        "        self.last_vals = vals\n",
        "        cur_info = str(self)\n",
        "\n",
        "        if not self.started:\n",
        "            self.started = True\n",
        "            if self.is_ipython:\n",
        "                print(self.display_id, '<- display_id')\n",
        "                display({'text/plain': ''}, display_id=self.display_id, raw=True)\n",
        "\n",
        "        self.display(cur_info)\n",
        "\n",
        "    def finalize(self):\n",
        "        if (not self.finalized) and (not self.is_ipython):\n",
        "            print(str(self))\n",
        "\n",
        "def progress_str(width, state):\n",
        "    progress = width * state\n",
        "    filled = int(math.floor(progress))\n",
        "\n",
        "    if filled < width:\n",
        "        remnant = str(int(math.floor((progress - filled) * 10.0)))\n",
        "        return '[' + '='* filled + remnant + ' ' * (width - filled - 1) + ']'\n",
        "    else:\n",
        "        return '[' + '=' * width + ']'\n",
        "\n",
        "class TimeEstimator:\n",
        "    def __init__(self, eta_threshold=0.001):\n",
        "        self.eta_threshold = eta_threshold\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.start_time = time.time()\n",
        "        self.cur_state = 0\n",
        "        self.est_finish_time = None\n",
        "        return self\n",
        "\n",
        "    def update(self, cur_state):\n",
        "        self.cur_state = cur_state\n",
        "        if self.cur_state >= self.eta_threshold:\n",
        "            self.est_finish_time = self.start_time + (time.time() - self.start_time) / self.cur_state\n",
        "\n",
        "    def __str__(self):\n",
        "        elapsed = str(datetime.timedelta(seconds=int(time.time() - self.start_time)))\n",
        "        if self.est_finish_time is not None:\n",
        "            eta = str(datetime.timedelta(seconds=int(self.est_finish_time - time.time())))\n",
        "        else:\n",
        "            eta = '?'\n",
        "\n",
        "        return f'[{elapsed}>{eta}]'\n",
        "\n",
        "\n",
        "class LiteProgressBar(pl.callbacks.ProgressBarBase):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.last_epoch = 0\n",
        "        self.pbar = StageProgressBar(width_function=get_width, display_id='ep{}'.format(0))\n",
        "        self.timer = TimeEstimator()\n",
        "        self.display_counter = 0\n",
        "\n",
        "    def disable(self):\n",
        "        self.enable = False\n",
        "\n",
        "    def on_train_epoch_start(self, *args, **kwargs):\n",
        "        super().on_train_epoch_start(*args, **kwargs)\n",
        "        self.timer.reset()\n",
        "        trainer = args[0]\n",
        "        log = copy.deepcopy(trainer.logged_metrics)\n",
        "        if 'epoch' in log:\n",
        "            log['Info/epoch'] = copy.deepcopy(log['epoch'])\n",
        "            del log['epoch']\n",
        "        log['Info/Mode'] = 'train'\n",
        "        log['Info/Progress'] = progress_str(15, 0)\n",
        "        log['Info/Time'] = str(self.timer)\n",
        "        self.pbar.update(log)\n",
        "        self.pbar.update(trainer.logged_metrics)\n",
        "\n",
        "    def on_train_epoch_end(self, *args, **kwargs):\n",
        "        super().on_train_epoch_end(*args, **kwargs)\n",
        "        trainer = args[0]\n",
        "        log = copy.deepcopy(trainer.logged_metrics)\n",
        "        if 'epoch' in log:\n",
        "            log['Info/epoch'] = copy.deepcopy(log['epoch'])\n",
        "            del log['epoch']\n",
        "        log['Info/Mode'] = 'train'\n",
        "        log['Info/Progress'] = progress_str(15, 1.0)\n",
        "        log['Info/Time'] = str(self.timer)\n",
        "        self.pbar.update(log)\n",
        "        self.pbar.update(trainer.logged_metrics)\n",
        "\n",
        "    def on_train_batch_end(self, *args, **kwargs):\n",
        "        super().on_train_batch_end(*args, **kwargs)\n",
        "        self.timer.update(float(self.train_batch_idx)/float(self.total_train_batches))\n",
        "        trainer = args[0]\n",
        "        log = copy.deepcopy(trainer.logged_metrics)\n",
        "        if 'epoch' in log:\n",
        "            log['Info/epoch'] = copy.deepcopy(log['epoch'])\n",
        "            del log['epoch']\n",
        "        log['Info/Mode'] = 'train'\n",
        "        log['Info/Progress'] = progress_str(15, float(self.train_batch_idx)/float(self.total_train_batches)) + ' ' + str(self.train_batch_idx) + ' / ' + str(self.total_train_batches)\n",
        "        log['Info/Time'] = str(self.timer)\n",
        "        self.pbar.update(log)\n",
        "\n",
        "    def on_validation_epoch_start(self, *args, **kwargs):\n",
        "        super().on_validation_epoch_start(*args, **kwargs)\n",
        "        self.timer.reset()\n",
        "        trainer = args[0]\n",
        "        log = trainer.logged_metrics\n",
        "        if 'epoch' in log:\n",
        "            log['Info/epoch'] = copy.deepcopy(log['epoch'])\n",
        "            del log['epoch']\n",
        "        log['Info/Mode'] = 'val'\n",
        "        log['Info/Progress'] = progress_str(15, 0)\n",
        "        log['Info/Time'] = str(self.timer)\n",
        "        self.pbar.update(log)\n",
        "        self.pbar.update(trainer.logged_metrics)\n",
        "\n",
        "    def on_validation_epoch_end(self, *args, **kwargs):\n",
        "        super().on_validation_epoch_end(*args, **kwargs)\n",
        "        trainer = args[0]\n",
        "        log = copy.deepcopy(trainer.logged_metrics)\n",
        "        if 'epoch' in log:\n",
        "            log['Info/epoch'] = copy.deepcopy(log['epoch'])\n",
        "            del log['epoch']\n",
        "        log['Info/Mode'] = 'val'\n",
        "        log['Info/Progress'] = progress_str(15, 1.0)\n",
        "        log['Info/Time'] = str(self.timer)\n",
        "        self.pbar.update(log)\n",
        "        self.pbar.update(trainer.logged_metrics)\n",
        "\n",
        "    def on_validation_batch_end(self, *args, **kwargs):\n",
        "        super().on_validation_batch_end(*args, **kwargs)\n",
        "        self.timer.update(float(self.val_batch_idx)/float(self.total_val_batches))\n",
        "        trainer = args[0]\n",
        "        log = copy.deepcopy(trainer.logged_metrics)\n",
        "        if 'epoch' in log:\n",
        "            log['Info/epoch'] = copy.deepcopy(log['epoch'])\n",
        "            del log['epoch']\n",
        "        log['Info/Mode'] = 'val'\n",
        "        log['Info/Progress'] = progress_str(15, float(self.val_batch_idx)/float(self.total_val_batches)) + ' ' + str(self.val_batch_idx) + ' / ' + str(self.total_val_batches)\n",
        "        log['Info/Time'] = str(self.timer)\n",
        "        self.pbar.update(log)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 459
        },
        "id": "Yp3WEqYJUZ87",
        "outputId": "4e47e102-16ee-48c0-e6f5-8400a4306b34"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "GPU available: True, used: True\n",
            "TPU available: False, using: 0 TPU cores\n",
            "IPU available: False, using: 0 IPUs\n",
            "LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]\n",
            "\n",
            "  | Name  | Type    | Params\n",
            "----------------------------------\n",
            "0 | model | Network | 144 K \n",
            "----------------------------------\n",
            "144 K     Trainable params\n",
            "0         Non-trainable params\n",
            "144 K     Total params\n",
            "0.578     Total estimated model params size (MB)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ep0 <- display_id\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "====================================================================================================\n",
              "\u001b[1;31m    loss \u001b[0;0m\u001b[3;31mtrain_step\u001b[0;0m: 0.015266866 | \u001b[3;31mvalid\u001b[0;0m: 0.018190 | \u001b[3;31mtrain_epoch\u001b[0;0m: 0.026026                          \n",
              "\u001b[1;32maccuracy \u001b[0;0m\u001b[3;32mtrain_step\u001b[0;0m: 0.9921875 | \u001b[3;32mvalid\u001b[0;0m: 0.993900 | \u001b[3;32mtrain_epoch\u001b[0;0m: 0.992171                            \n",
              "\u001b[1;33m    Info \u001b[0;0m\u001b[3;33mMode\u001b[0;0m: train | \u001b[3;33mProgress\u001b[0;0m: [===5           ] 112 / 468 | \u001b[3;33mTime\u001b[0;0m: [0:00:09>0:00:30]              \n",
              "===================================================================================================="
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/torch/utils/data/dataloader.py:481: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  cpuset_checked))\n",
            "/usr/local/lib/python3.7/dist-packages/pytorch_lightning/trainer/trainer.py:688: UserWarning: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
            "  rank_zero_warn(\"Detected KeyboardInterrupt, attempting graceful shutdown...\")\n"
          ]
        }
      ],
      "source": [
        "model = PLModel(Network())\n",
        "\n",
        "logger = pl.loggers.TensorBoardLogger(\n",
        "    'lightning_logs',\n",
        "    name='my_model'\n",
        ")\n",
        "\n",
        "trainer = pl.Trainer(\n",
        "    gpus=1,\n",
        "    logger=logger,\n",
        "    callbacks=[LiteProgressBar()])\n",
        "\n",
        "trainer.fit(\n",
        "    model, train_loader,\n",
        "    val_dataloaders=test_loader)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iihZJDSeH9tk"
      },
      "source": [
        "## 6. Preparing to deploy to a mobile device"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KNteYQJzOJzs"
      },
      "source": [
        "### 6.1. Instructions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GKHQ_y9eIJiV"
      },
      "source": [
        "In this assignment, we will use PyTorch Mobile to deploy the model to a mobile device.\n",
        "\n",
        "In order to rebuild the model for use on the device, you must do the following:\n",
        "\n",
        "1. Put the model in `eval` mode. For the model presented in the task, this is not necessary, but for models that have normalization layers and more complex tricks, it is necessary.\n",
        "\n",
        "2. Simulate example input signal: generate a signal of the same size as the expected signal that will be input to the model in the application. In our case, this will be a tensor of size `1x1x28x28`:\n",
        "     * 1 image\n",
        "     * 1 channel (grayscale)\n",
        "     * 28x28 -- image resolution\n",
        "\n",
        "\n",
        "3. Trace the model using the `torch.jit.trace` method using the generated random input.\n",
        "\n",
        "4. Optimize the traced model for use in mobile devices using the `torch.utils.mobile_optimizer.optimize_for_mobile` method.\n",
        "\n",
        "5. Save the optimized model using the `_save_for_lite_interpreter` method of the model. The final model should be named `model.ptl`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SuF1pd1LOQOL"
      },
      "source": [
        "### 6.2. Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qPatgdw9ylvM"
      },
      "outputs": [],
      "source": [
        "from torch.utils.mobile_optimizer import optimize_for_mobile\n",
        "\n",
        "... # YOUR CODE HERE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_UaEoDPMOWCl"
      },
      "source": [
        "### 6.3. Application assembly"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pjrjfsSgP0YD"
      },
      "source": [
        "It remains to build the application. Since in this course I am very interested in how exactly the application is written for Android, we will simply add the model to the already finished shell. The application code is available in the Pytorch Mobile Tutorial for [ViT 4 MNIST](https://github.com/pytorch/android-demo-app/blob/master/ViT4MNIST/).\n",
        "\n",
        "The following few lines of code copy the wrapper for the model from github, wrap the model in this wrapper and prepare a zip archive that you just need to download, unpack and run in Android Studio. After that, you can test how the application works in emulators, as well as run it on your phone."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I-V9forQ07QY",
        "outputId": "7ac3c611-2575-4c4f-faa0-6bea5e8c406f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'android-demo-app' already exists and is not an empty directory.\n",
            "updating: android-demo-app/ViT4MNIST/ (stored 0%)\n",
            "updating: android-demo-app/ViT4MNIST/convert_deit.py (deflated 45%)\n",
            "updating: android-demo-app/ViT4MNIST/screenshot3.png (deflated 10%)\n",
            "updating: android-demo-app/ViT4MNIST/build.gradle (deflated 47%)\n",
            "updating: android-demo-app/ViT4MNIST/app/ (stored 0%)\n",
            "updating: android-demo-app/ViT4MNIST/app/build.gradle (deflated 58%)\n",
            "updating: android-demo-app/ViT4MNIST/app/src/ (stored 0%)\n",
            "updating: android-demo-app/ViT4MNIST/app/src/main/ (stored 0%)\n",
            "updating: android-demo-app/ViT4MNIST/app/src/main/AndroidManifest.xml (deflated 54%)\n",
            "updating: android-demo-app/ViT4MNIST/app/src/main/assets/ (stored 0%)\n",
            "updating: android-demo-app/ViT4MNIST/app/src/main/assets/vit4mnist.ptl (deflated 8%)\n",
            "updating: android-demo-app/ViT4MNIST/app/src/main/java/ (stored 0%)\n",
            "updating: android-demo-app/ViT4MNIST/app/src/main/java/org/ (stored 0%)\n",
            "updating: android-demo-app/ViT4MNIST/app/src/main/java/org/pytorch/ (stored 0%)\n",
            "updating: android-demo-app/ViT4MNIST/app/src/main/java/org/pytorch/demo/ (stored 0%)\n",
            "updating: android-demo-app/ViT4MNIST/app/src/main/java/org/pytorch/demo/vit4mnist/ (stored 0%)\n",
            "updating: android-demo-app/ViT4MNIST/app/src/main/java/org/pytorch/demo/vit4mnist/HandWrittenDigitView.java (deflated 69%)\n",
            "updating: android-demo-app/ViT4MNIST/app/src/main/java/org/pytorch/demo/vit4mnist/MainActivity.java (deflated 68%)\n",
            "updating: android-demo-app/ViT4MNIST/app/src/main/res/ (stored 0%)\n",
            "updating: android-demo-app/ViT4MNIST/app/src/main/res/drawable-v24/ (stored 0%)\n",
            "updating: android-demo-app/ViT4MNIST/app/src/main/res/drawable-v24/ic_launcher_foreground.xml (deflated 63%)\n",
            "updating: android-demo-app/ViT4MNIST/app/src/main/res/drawable/ (stored 0%)\n",
            "updating: android-demo-app/ViT4MNIST/app/src/main/res/drawable/ic_launcher_background.xml (deflated 93%)\n",
            "updating: android-demo-app/ViT4MNIST/app/src/main/res/mipmap-hdpi/ (stored 0%)\n",
            "updating: android-demo-app/ViT4MNIST/app/src/main/res/mipmap-hdpi/ic_launcher_round.png (stored 0%)\n",
            "updating: android-demo-app/ViT4MNIST/app/src/main/res/mipmap-hdpi/ic_launcher.png (deflated 1%)\n",
            "updating: android-demo-app/ViT4MNIST/app/src/main/res/layout/ (stored 0%)\n",
            "updating: android-demo-app/ViT4MNIST/app/src/main/res/layout/activity_main.xml (deflated 78%)\n",
            "updating: android-demo-app/ViT4MNIST/app/src/main/res/mipmap-xhdpi/ (stored 0%)\n",
            "updating: android-demo-app/ViT4MNIST/app/src/main/res/mipmap-xhdpi/ic_launcher_round.png (stored 0%)\n",
            "updating: android-demo-app/ViT4MNIST/app/src/main/res/mipmap-xhdpi/ic_launcher.png (deflated 0%)\n",
            "updating: android-demo-app/ViT4MNIST/app/src/main/res/mipmap-mdpi/ (stored 0%)\n",
            "updating: android-demo-app/ViT4MNIST/app/src/main/res/mipmap-mdpi/ic_launcher_round.png (stored 0%)\n",
            "updating: android-demo-app/ViT4MNIST/app/src/main/res/mipmap-mdpi/ic_launcher.png (stored 0%)\n",
            "updating: android-demo-app/ViT4MNIST/app/src/main/res/mipmap-xxxhdpi/ (stored 0%)\n",
            "updating: android-demo-app/ViT4MNIST/app/src/main/res/mipmap-xxxhdpi/ic_launcher_round.png (deflated 0%)\n",
            "updating: android-demo-app/ViT4MNIST/app/src/main/res/mipmap-xxxhdpi/ic_launcher.png (deflated 0%)\n",
            "updating: android-demo-app/ViT4MNIST/app/src/main/res/mipmap-xxhdpi/ (stored 0%)\n",
            "updating: android-demo-app/ViT4MNIST/app/src/main/res/mipmap-xxhdpi/ic_launcher_round.png (deflated 0%)\n",
            "updating: android-demo-app/ViT4MNIST/app/src/main/res/mipmap-xxhdpi/ic_launcher.png (deflated 0%)\n",
            "updating: android-demo-app/ViT4MNIST/app/src/main/res/values/ (stored 0%)\n",
            "updating: android-demo-app/ViT4MNIST/app/src/main/res/values/colors.xml (deflated 39%)\n",
            "updating: android-demo-app/ViT4MNIST/app/src/main/res/values/styles.xml (deflated 52%)\n",
            "updating: android-demo-app/ViT4MNIST/app/src/main/res/values/strings.xml (deflated 20%)\n",
            "updating: android-demo-app/ViT4MNIST/gradle.properties (deflated 48%)\n",
            "updating: android-demo-app/ViT4MNIST/gradlew.bat (deflated 60%)\n",
            "updating: android-demo-app/ViT4MNIST/screenshot1.png (deflated 10%)\n",
            "updating: android-demo-app/ViT4MNIST/mnist_vit.py (deflated 64%)\n",
            "updating: android-demo-app/ViT4MNIST/README.md (deflated 58%)\n",
            "updating: android-demo-app/ViT4MNIST/settings.gradle (stored 0%)\n",
            "updating: android-demo-app/ViT4MNIST/vit_pytorch.py (deflated 68%)\n",
            "updating: android-demo-app/ViT4MNIST/gradlew (deflated 61%)\n",
            "updating: android-demo-app/ViT4MNIST/screenshot2.png (deflated 10%)\n",
            "updating: android-demo-app/ViT4MNIST/gradle/ (stored 0%)\n",
            "updating: android-demo-app/ViT4MNIST/gradle/wrapper/ (stored 0%)\n",
            "updating: android-demo-app/ViT4MNIST/gradle/wrapper/gradle-wrapper.properties (deflated 35%)\n",
            "updating: android-demo-app/ViT4MNIST/gradle/wrapper/gradle-wrapper.jar (deflated 10%)\n"
          ]
        }
      ],
      "source": [
        "!git clone https://github.com/pytorch/android-demo-app\n",
        "!cp ./model.ptl ./android-demo-app/ViT4MNIST/app/src/main/assets/vit4mnist.ptl\n",
        "!zip -r android.zip ./android-demo-app/ViT4MNIST/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FUIaJTJ3scDT"
      },
      "source": [
        "## 7. Application launch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dSplGbSXrcLF"
      },
      "source": [
        "In order to test the application on an Android phone, download Android Studio, unpack and open the project. It will be necessary to install component updates and connect your phone. After building and running, you can evaluate how your application works.\n",
        "\n",
        "If you don't have an Android phone, you can use the emulator that comes with Android Studio. Of course, it's not that interesting, but it's better than nothing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PwdzJH2osQJ4"
      },
      "source": [
        "## 8. Additional tasks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0uq2YVhdsoM3"
      },
      "source": [
        "The resulting application somehow works. This quite accurately describes the quality of the application, despite the rather high test score. In order to improve the quality of work, you can try the following:\n",
        "\n",
        "1. Instead of a fully connected neural network, use a convolutional one. For example, you can use a fairly simple LeNet architecture\n",
        "\n",
        "2. To further increase the stability of the model, you can use more diverse augmentations. For example, you can use RandomResizedCrop, rotations, adding a little noise."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "qWt0iuJbKdwY",
        "leg6cl0LTvgE",
        "0rIczAZDQ5ed",
        "oOotwBlCq1yH",
        "qLB9pVGrdsRP",
        "f_0GI7bIh_oc",
        "qqVoQYo3Fiwe",
        "ALud-qiffruc",
        "bFjF5lk7MFTT",
        "dtOO1GnOOEWj",
        "MS9UeP4eOSCo",
        "H4nE3JOaOeIM",
        "jpP4pHsjg-0Y",
        "38Az-VUQOvAU",
        "JmwyUvjCeWNv",
        "eD-5rFJ0eakB",
        "-9URMzgnyCJa",
        "KNteYQJzOJzs",
        "SuF1pd1LOQOL",
        "FUIaJTJ3scDT"
      ],
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "torch",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.9"
    },
    "vscode": {
      "interpreter": {
        "hash": "625a3e9bb539bddba3637b86871d7600e463f0e3e5553319a9ee64ea984715f3"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}